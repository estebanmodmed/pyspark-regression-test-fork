from datetime import date, datetime, timezone
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.types import (
    BooleanType,
    IntegerType,
    FloatType,
    DoubleType,
    DateType,
    TimestampType,
    StringType,
    StructType,
    StructField,
)
from pyspark_regression.regression import RegressionTest

spark = SparkSession.builder.getOrCreate()
spark.conf.set("spark.sql.shuffle.partitions", "8")

schema = StructType(
    [
        StructField("id", IntegerType()),
        StructField("attr_str", StringType()),
        StructField("attr_bool", BooleanType()),
        StructField("attr_int", IntegerType()),
        StructField("attr_double", DoubleType()),
        StructField("attr_float", FloatType()),
        StructField("attr_date", DateType()),
        StructField("attr_timestamp", TimestampType()),
    ]
)

df_old = spark.createDataFrame(
    [
        (
            1,
            "a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            2,
            "test",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            3,
            "tes",
            None,
            2,
            1.0,
            2.00001,
            date(2020, 1, 2),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            4,
            "a",
            True,
            1,
            1.0,
            2.10000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 16, 0, 0, 0, timezone.utc),
        ),
        (
            5,
            "A",
            False,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            6,
            "a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            7,
            "a ",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            8,
            "a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            9,
            " a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            9,
            " a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            9,
            " a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            11,
            " a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
    ],
    schema=schema,
)

df_new = spark.createDataFrame(
    [
        (
            1,
            "a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            2,
            "tes",
            None,
            2,
            1.0,
            2.00001,
            date(2020, 1, 2),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            3,
            "test",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            4,
            "A",
            False,
            1,
            1.0,
            2.10000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            5,
            "a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 16, 0, 0, 0, timezone.utc),
        ),
        (
            6,
            "a ",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            7,
            "a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            8,
            " a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            9,
            "a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            9,
            "a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
        (
            10,
            "a",
            True,
            1,
            1.0,
            2.00000,
            date(2020, 1, 1),
            datetime(2020, 1, 1, 12, 0, 0, 0, timezone.utc),
        ),
    ],
    schema=schema,
)

r = RegressionTest(
    df_old=df_old,
    df_new=(
        df_new.withColumn("attr_date_new", F.col("attr_date"))
        .drop("attr_date")
        .withColumn("attr_int", F.col("attr_int").cast(DoubleType()))
    ),
    pk="id",
)

print(r.report)
